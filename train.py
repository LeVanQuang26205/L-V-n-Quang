import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
from torchvision.datasets import ImageFolder
import os
import time
import numpy as np

# --- C·∫§U H√åNH D·ª∞ √ÅN KH·∫®U TRANG ƒêEN ---
DATA_DIR = 'DataMask'
MODEL_SAVE_PATH = 'mask_angle_2class_vgg16.pth'
NUM_EPOCHS = 30  # TƒÉng s·ªë Epoch ƒë·ªÉ c√≥ th·ªùi gian D·ª´ng s·ªõm (t·ªëi ∆∞u nh·∫•t)
BATCH_SIZE = 32
LEARNING_RATE = 0.00001
IMAGE_SIZE = 224
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# --- C·∫§U H√åNH CHO ·ªîN ƒê·ªäNH V√Ä H·ªåC LI√äN T·ª§C ---
PATIENCE = 7  # S·ªë l∆∞·ª£ng epoch ch·ªù Val Loss kh√¥ng c·∫£i thi·ªán tr∆∞·ªõc khi GI·∫¢M LR (LR Scheduler)
ES_PATIENCE = 15  # S·ªë l∆∞·ª£ng epoch ch·ªù Val Acc kh√¥ng c·∫£i thi·ªán tr∆∞·ªõc khi D·ª™NG S·ªöM (Early Stopping)


# --- H√ÄM T·∫¢I D·ªÆ LI·ªÜU V√Ä TƒÇNG C∆Ø·ªúNG ---
def get_data_loaders(data_dir, batch_size, img_size):
    """T·∫£i v√† √°p d·ª•ng bi·∫øn ƒë·ªïi cho t·∫≠p Train v√† Val, bao g·ªìm Data Augmentation m·∫°nh."""

    # Data Augmentation m·∫°nh m·∫Ω cho t·∫≠p TRAIN
    train_transforms = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),
        transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Bi·∫øn ƒë·ªïi ƒë∆°n gi·∫£n cho t·∫≠p VAL
    val_transforms = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    train_dataset = ImageFolder(root=os.path.join(data_dir, 'train'), transform=train_transforms)
    val_dataset = ImageFolder(root=os.path.join(data_dir, 'val'), transform=val_transforms)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

    return train_loader, val_loader, train_dataset.classes


# --- H√ÄM X√ÇY D·ª∞NG M√î H√åNH V√Ä T·∫¢I TR·ªåNG S·ªê ---
def build_model(num_classes, model_save_path, device):
    """X√¢y d·ª±ng VGG16 v√† t·∫£i tr·ªçng s·ªë ƒë√£ l∆∞u n·∫øu t·ªìn t·∫°i ƒë·ªÉ h·ªçc li√™n t·ª•c."""
    model = models.vgg16(weights='DEFAULT')

    # ƒê√≥ng bƒÉng c√°c l·ªõp Convolution (CONV)
    for param in model.parameters():
        param.requires_grad = False

    # Thay th·∫ø l·ªõp ph√¢n lo·∫°i (FC layer) cu·ªëi c√πng
    num_ftrs = model.classifier[6].in_features
    model.classifier[6] = nn.Linear(num_ftrs, num_classes)

    # T·∫¢I TR·ªåNG S·ªê ƒê√É HU·∫§N LUY·ªÜN (H·ªåC LI√äN T·ª§C)
    if os.path.exists(model_save_path):
        print(f"‚úÖ Ph√°t hi·ªán m√¥ h√¨nh ƒë√£ l∆∞u, t·∫£i tr·ªçng s·ªë t·ª´ {model_save_path} ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán...")
        model.load_state_dict(torch.load(model_save_path, map_location=device))
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh ƒë√£ l∆∞u. B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán t·ª´ ƒë·∫ßu.")

    model = model.to(device)
    return model


# --- H√ÄM HU·∫§N LUY·ªÜN CH√çNH ---
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):
    since = time.time()
    best_acc = 0.0

    # Logic D·ª´ng S·ªõm
    early_stop_counter = 0

    for epoch in range(num_epochs):
        print(f"Epoch {epoch}/{num_epochs - 1}")
        print("-" * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
                data_loader = train_loader
            else:
                model.eval()
                data_loader = val_loader

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in data_loader:
                inputs = inputs.to(DEVICE)
                labels = labels.to(DEVICE)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(data_loader.dataset)
            epoch_acc = running_corrects.double() / len(data_loader.dataset)

            print(f"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}")

            # LOGIC D·ª™NG S·ªöM & ƒêI·ªÄU CH·ªàNH LR
            if phase == 'val':
                # B·ªô ƒëi·ªÅu ch·ªânh LR (theo d√µi Val Loss)
                scheduler.step(epoch_loss)

                # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t (theo d√µi Val Acc)
                if epoch_acc > best_acc:
                    best_acc = epoch_acc
                    torch.save(model.state_dict(), MODEL_SAVE_PATH)
                    print(f"*** L∆∞u m√¥ h√¨nh m·ªõi (Acc: {best_acc:.4f}) t·∫°i {MODEL_SAVE_PATH} ***")
                    early_stop_counter = 0  # ƒê·∫∑t l·∫°i b·ªô ƒë·∫øm khi m√¥ h√¨nh c·∫£i thi·ªán
                else:
                    early_stop_counter += 1

                # Logic D·ª´ng S·ªõm (Early Stopping)
                if early_stop_counter >= ES_PATIENCE:
                    print(f"\nüõë D·ª™NG S·ªöM: Val Acc kh√¥ng c·∫£i thi·ªán sau {ES_PATIENCE} Epoch. K·∫øt th√∫c hu·∫•n luy·ªán.")
                    time_elapsed = time.time() - since
                    print(f"Hu·∫•n luy·ªán ho√†n t·∫•t trong {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s")
                    print(f"ƒê·ªô ch√≠nh x√°c t·ªët nh·∫•t tr√™n t·∫≠p Val: {best_acc:.4f}")
                    return

    time_elapsed = time.time() - since
    print(f"Hu·∫•n luy·ªán ho√†n t·∫•t trong {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s")
    print(f"ƒê·ªô ch√≠nh x√°c t·ªët nh·∫•t tr√™n t·∫≠p Val: {best_acc:.4f}")


if __name__ == '__main__':
    train_loader, val_loader, class_names = get_data_loaders(DATA_DIR, BATCH_SIZE, IMAGE_SIZE)
    num_classes = len(class_names)

    print(f"M√¥ h√¨nh s·∫Ω ph√¢n lo·∫°i {num_classes} l·ªõp: {class_names}")

    if num_classes != 2:
        print(
            f"‚ùå L·ªñI: D·ª± √°n n√†y y√™u c·∫ßu 2 l·ªõp, nh∆∞ng t√¨m th·∫•y {num_classes}. Vui l√≤ng ki·ªÉm tra l·∫°i folder DataMask/train v√† DataMask/val.")
        exit()

    # X√¢y d·ª±ng m√¥ h√¨nh v√† T·∫¢I TR·ªåNG S·ªê
    model = build_model(num_classes, MODEL_SAVE_PATH, DEVICE)

    # T·ªëi ∆∞u h√≥a ch·ªâ c√°c tham s·ªë c·ªßa l·ªõp ph√¢n lo·∫°i cu·ªëi c√πng
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)

    # B·ªò ƒêI·ªÄU CH·ªàNH T·ªêC ƒê·ªò H·ªåC (LR Scheduler) - ƒê√É S·ª¨A L·ªñI VERBOSE
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.1,
        patience=PATIENCE
        # ƒê√£ x√≥a verbose=True ƒë·ªÉ kh·∫Øc ph·ª•c l·ªói
    )

    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS)